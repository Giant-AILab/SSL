![image](https://github.com/user-attachments/assets/77a9b642-f41b-401a-85a4-601f4ca9754e)# Self-Supervised Learning of Deviation in Latent Representation for Co-speech Gesture Video Generation

Huan Yang*, Jiahui Chen*, Chaofan Ding, Runhua Shi, Siyu Xiong, Qingqi Hong, Xiaoqi Mo, Xinhan Di

Giant Interactive Group Inc & AI Lab, Xiamen University

*Denotes Equal Contribution

in _ECCV 2024 workshop_

![image](https://github.com/user-attachments/assets/c49ae05a-b3f2-4ef8-8524-b43410e7fc69)

## Pipeline

 Co-speech gesture video generation pipeline of our proposed method consists of three main components: 1) the latent deviation extractor (orange) 2) the latent deviation decoder (blue) 3) the latent motion diffusion (green).

![image](https://github.com/user-attachments/assets/5723b685-2fb8-4ecf-ab7c-309f83bb07b7)

## Visual comparison

![image](https://github.com/user-attachments/assets/9a7cfca4-d46b-4fc4-9df2-fd9d9e2aceed)

![image](https://github.com/user-attachments/assets/db1292fc-55db-4be7-ae59-74d8cdd86dfd)

## Generated video comparison

[![](https://i.ytimg.com/vi/HPRfwyL4vMc/maxresdefault.jpg)](https://youtu.be/HPRfwyL4vMc "")

[![](https://i.ytimg.com/vi/U8i7QRGOQGo/maxresdefault.jpg)](https://youtu.be/U8i7QRGOQGo "")

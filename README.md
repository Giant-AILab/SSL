# Self-Supervised Learning of Deviation in Latent Representation for Co-speech Gesture Video Generation

Huan Yang*<sup>1</sup>, Jiahui Chen*<sup>1, 2</sup>, Chaofan Ding<sup>1</sup>, Runhua Shi<sup>1</sup>, Siyu Xiong<sup>1</sup>, Qingqi Hong<sup>2</sup>, Xiaoqi Mo<sup>1</sup>, Xinhan Di<sup>1</sup>

1 Giant Interactive Group Inc & AI Lab, 2 Xiamen University

*Denotes Equal Contribution

in _ECCV 2024 workshop_

![image](https://github.com/user-attachments/assets/c49ae05a-b3f2-4ef8-8524-b43410e7fc69)

## Pipeline

 Co-speech gesture video generation pipeline of our proposed method consists of three main components: 1) the latent deviation extractor (orange) 2) the latent deviation decoder (blue) 3) the latent motion diffusion (green).

![image](https://github.com/user-attachments/assets/5723b685-2fb8-4ecf-ab7c-309f83bb07b7)

## Visual comparison

![image](https://github.com/user-attachments/assets/9a7cfca4-d46b-4fc4-9df2-fd9d9e2aceed)

![image](https://github.com/user-attachments/assets/db1292fc-55db-4be7-ae59-74d8cdd86dfd)

## Generated video comparison

[![](https://i.ytimg.com/vi/HPRfwyL4vMc/maxresdefault.jpg)](https://youtu.be/HPRfwyL4vMc "")

[![](https://i.ytimg.com/vi/U8i7QRGOQGo/maxresdefault.jpg)](https://youtu.be/U8i7QRGOQGo "")
